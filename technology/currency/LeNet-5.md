---
title: LeNet-5
mathjax: true
date: 2022-10-04 20:24:55
tags: 卷积神将网络
categories: 神经网络
cover:
---

## 简介

- LeNet-5由LeCun等人提出于1998年提出，是一种用于手写体字符识别的非常高效的卷积神经网络。出自论文《Gradient-Based Learning Applied to Document Recognition》
- http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf

## 结构

- LeNet-5网络示意图

  ![](https://s3.bmp.ovh/imgs/2022/10/04/819b3373e991f235.png)

- C1层：该层是一个卷积层。使用6个大小为5x5的卷积核，步长为1，对输入层进行卷积运算，特征图尺寸为32-5+1=28，因此产生6个大小为28x28的特征图。这么做够防止原图像输入的信息掉到卷积核边界之外。

- S2层：该层是一个池化层（pooling，也称为下采样层）。这里采用max_pool（最大池化），池化的size定为2x2，经池化后得到6个14x14的特征图，作为下一层神经元的输入。

- C3层：该层仍为一个卷积层，我们选用大小为5x5的16种不同的卷积核。这里需要注意：C3中的每个特征图，都是S2中的所有6个或其中几个特征图进行加权组合得到的。输出为16个10x10的特征图。

- S4层：该层仍为一个池化层，size为2x2，仍采用max_pool。最后输出16个5x5的特征图，神经元个数也减少至16x5x5=400。

- C5层：该层我们继续用5x5的卷积核对S4层的输出进行卷积，卷积核数量增加至120。这样C5层的输出图片大小为5-5+1=1。最终输出120个1x1的特征图。这里实际上是与S4全连接了，但仍将其标为卷积层，原因是如果LeNet-5的输入图片尺寸变大，其他保持不变，那该层特征图的维数也会大于1x1。

- F6层：该层与C5层全连接，输出84张特征图。

- 输出层：该层与F6层全连接，输出长度为10的张量，代表所抽取的特征属于哪个类别。（例如[0,0,0,1,0,0,0,0,0,0]的张量，1在index=3的位置，故该张量代表的图片属于第三类）



## 明细

### 名词解释

- padding：在图像四周填充零的层数
- stride：卷积核移动的步长

### C1层

- C1 层是卷积层，使用 6 个 5×5 大小的卷积核，padding=0，stride=1进行卷积，得到 6 个 28×28 大小的特征图：32-5+1=28

- C1层原理示意

  ![](https://s3.bmp.ovh/imgs/2022/10/04/392c16739f6bc405.png)

- **参数个数**：(5x5+1)x6=156，其中5x5为卷积核的25个参数w，1为偏置项b。

  **连接数**：156x28x28=122304，其中156为单次卷积过程连线数，28x28为输出特征层，每一个像素都由前面卷积得到，即总共经历28x28次卷积。

### S2层

- S2 层是降采样层，使用 6 个 2×2 大小的卷积核进行池化，padding=0，stride=2，得到 6 个 14×14 大小的特征图：28/2=14。

- S2 层其实相当于降采样层+激活层。先是降采样，然后激活函数 sigmoid 非线性输出。先对 C1 层 2x2 的视野求和，然后进入激活函数，即：
  $$
  sigmoid(w·\sum_{i = 1}^4x_i + b)
  $$

- S2层示意图

  ![](https://s3.bmp.ovh/imgs/2022/10/04/97039d914f703b7e.png)

- **参数个数**：(1+1)x6=12，其中第一个 1 为池化对应的 2x2 感受野中最大的那个数的权重 w，第二个 1 为偏置 b。

  **连接数**：(2x2+1)x6x14x14= 5880，虽然只选取 2x2 感受野之和，但也存在 2x2 的连接数，1 为偏置项的连接，14x14 为输出特征层，每一个像素都由前面卷积得到，即总共经历 14x14 次卷积。



### C3层

- C3 层是卷积层，使用 16 个 5×5xn 大小的卷积核，padding=0，stride=1 进行卷积，得到 16 个 10×10 大小的特征图：14-5+1=10。

- 16 个卷积核并不是都与 S2 的 6 个通道层进行卷积操作，如下图所示，C3 的前六个特征图（0,1,2,3,4,5）由 S2 的相邻三个特征图作为输入，对应的卷积核尺寸为：5x5x3；接下来的 6 个特征图（6,7,8,9,10,11）由 S2 的相邻四个特征图作为输入对应的卷积核尺寸为：5x5x4；接下来的 3 个特征图（12,13,14）号特征图由 S2 间断的四个特征图作为输入对应的卷积核尺寸为：5x5x4；最后的 15 号特征图由 S2 全部(6 个)特征图作为输入，对应的卷积核尺寸为：5x5x6。

- C3输入示意图

  ![](https://s3.bmp.ovh/imgs/2022/10/04/66e1b30ae1a7e968.png)

- 值得注意的是，卷积核是 5×5 且具有 3 个通道，每个通道各不相同，这也是下面计算时 5*5 后面还要乘以3,4,6的原因。这是多通道卷积的计算方法。

- C3层示意图

  ![](https://s3.bmp.ovh/imgs/2022/10/04/7ae9f260059019a9.png)

- **参数个数**：(5x5x3+1)x6+(5x5x4+1)x6+(5x5x4+1)x3+(5x5x6+1)x1=1516。

  **连接数**：1516x10x10 = 151600。10x10为输出特征层，每一个像素都由前面卷积得到，即总共经历10x10次卷积。

### S4层

- S4 层与 S2 一样也是降采样层，使用 16 个 2×2 大小的卷积核进行池化，padding=0，stride=2，得到 16 个 5×5 大小的特征图：10/2=5。

- **参数个数**：(1+1)*16=32。

  **连接数**：(2*2+1)*16*5*5= 2000。

### C5层

- C5 层是卷积层，使用 120 个 5×5x16 大小的卷积核，padding=0，stride=1进行卷积，得到 120 个 1×1 大小的特征图：5-5+1=1。即相当于 120 个神经元的全连接层。

- 值得注意的是，与C3层不同，这里120个卷积核都与S4的16个通道层进行卷积操作。

- **参数个数**：(5*5*16+1)*120=48120。

  **连接数**：48120*1*1=48120。

### F6层

- F6 是全连接层，共有 84 个神经元，与 C5 层进行全连接，即每个神经元都与 C5 层的 120 个特征图相连。计算输入向量和权重向量之间的点积，再加上一个偏置，结果通过 sigmoid 函数输出。

- F6 层有 84 个节点，对应于一个 7x12 的比特图，-1 表示白色，1 表示黑色，这样每个符号的比特图的黑白色就对应于一个编码。该层的训练参数和连接数是(120 + 1)x84=10164。

- **参数个数**：(120+1)x84=10164。

  **连接数**：(120+1)x84=10164。

### 输出层

- 最后的 Output 层也是全连接层，是 Gaussian Connections，采用了 RBF 函数（即径向欧式距离函数），计算输入向量和参数向量之间的欧式距离（目前已经被Softmax 取代）。

- Output 层共有 10 个节点，分别代表数字 0 到 9。假设x是上一层的输入，y 是 RBF的输出，则 RBF 输出的计算方式是：

$$
y_i = \sum_{i=0}^{83}(x_j - w_{ij})^2
$$

- 上式中 i 取值从 0 到 9，j 取值从 0 到 7x12-1，w 为参数。RBF 输出的值越接近于 0，则越接近于 i，即越接近于 i 的 ASCII 编码图，表示当前网络输入的识别结果是字符 i。



## 总结

- 论文看不懂

